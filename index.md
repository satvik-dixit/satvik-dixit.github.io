---
layout: default
---

![Profile Picture](/assets/img/profile.jpg){:.profile-picture}

I am a master's student at Carnegie Mellon University. I work with Professor [Bhiksha Raj](https://cmu-mlsp.github.io/team/bhiksha_raj) on Speech Processing and Audio Language Models, and Professor [Chris Donahue](https://chrisdonahue.com/) on Text-to-Audio Models. My research interests focus on speech/audio processing and LLMs.

Previously, I worked at MIT with Professor [Satrajit Ghosh](https://sensein.group/) on the explainability of self-supervised learning (SSL) embeddings, such as WavLM, for speech emotion recognition. I have also worked at [EPFL](https://www.epfl.ch/labs/lcav/people/martin-vetterli/) on room acoustics simulation. I completed my undergraduate degree in Electrical Engineering at IIT Delhi, where my concentration was on signals and ML. My undergraduate thesis was on the classification of neuron types based on EEG signals. 

## Education

- M.S., Computer Engineering
  - Carnegie Mellon University
  - August 2023 - December 2024 (Expected)
- B. Tech., Electrical Engineering 
  - Indian Institute of Technology, Delhi
  - August 2019 - August 2023


## Selected Publications

<div class="publication">
  <img src="/assets/img/paper_1.jpg" alt="Speech Emotion Recognition Preview" class="publication-image">
  <div class="publication-content">
    <h4>Explaining Deep Learning Embeddings for Speech Emotion Recognition by Predicting Interpretable Acoustic Features</h4>
    <p><strong>Satvik Dixit</strong>, Daniel M. Low, Gasser Elbanna, Fabio Catania, Satrajit S. Ghosh</p>
    <ul>
      <li>Submitted to ICASSP 2025 (Main)</li>
      <li>[[Paper](https://www.arxiv.org/abs/2409.09511)]</li>
      <li>[[Code](https://github.com/satvik-dixit/explainability_SER/tree/main)]</li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_2.jpg" alt="Speaker Representations Preview" class="publication-image">
  <div class="publication-content">
    <h4>Improving Speaker Representations Using Contrastive Losses on Multi-scale Features</h4>
    <p><strong>Satvik Dixit</strong>, Massa Baali, Rita Singh, Bhiksha Raj</p>
    <ul>
      <li>Submitted to ICASSP 2025 (Main)</li>
      <li>[[Paper](https://arxiv.org/abs/2410.05037)]</li>
      <li>[[Code](https://github.com/satvik-dixit/MFCon/tree/main)]</li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_3.jpg" alt="Vision Language Models Preview" class="publication-image">
  <div class="publication-content">
    <h4>Vision Language Models Are Few-Shot Audio Spectrogram Classifiers</h4>
    <p><strong>Satvik Dixit</strong>, Laurie Heller, Chris Donahue</p>
    <ul>
      <li>Accepted at NeuRIPS Audio Imagination Workshop</li>
      <li>[[Paper](https://openreview.net/pdf?id=RnBAclRKOC)]</li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_4.jpg" alt="MACE Preview" class="publication-image">
  <div class="publication-content">
    <h4>MACE: Leveraging Audio for Evaluating Audio Captioning Systems</h4>
    <p><strong>Satvik Dixit</strong>, Soham Deshmukh, Bhiksha Raj</p>
    <ul>
      <li>Submitted to ICASSP 2025 Speech and Audio Language Models (SALMA) Workshop</li>
      <li>[[Code](https://github.com/satvik-dixit/mace/tree/main)]</li>
    </ul>
  </div>
</div>

<br>
<br>







 



  






