---
layout: default
---

I am a master's student at Carnegie Mellon University. I work with Professor [Bhiksha Raj](https://cmu-mlsp.github.io/team/bhiksha_raj) on Speech Processing and Audio Language Models and Professor [Chris Donahue](https://chrisdonahue.com/) on Text-to-Audio Models. My research interests focus on speech/audio processing and LLMs.

Previously, I worked with Dr [Satrajit Ghosh](https://sensein.group/) at MIT on the explainability of self-supervised learning (SSL) embeddings, such as WavLM, for speech emotion recognition. I have also worked at [EPFL](https://www.epfl.ch/labs/lcav/people/martin-vetterli/) on room acoustics simulation. I completed my undergraduate degree in Electrical Engineering at IIT Delhi, where my concentration was on signals processing and ML.

## Education

- M.S., Computer Engineering
  - Carnegie Mellon University
  - August 2023 - December 2024 (Expected)
- B. Tech., Electrical Engineering 
  - Indian Institute of Technology, Delhi
  - August 2019 - August 2023


## Publications and Preprints

<div class="publication">
  <img src="/assets/img/paper_4.png" alt="MACE Preview" class="publication-image" style="width: 300px;">
  <div class="publication-content">
    <h4>MACE: Leveraging Audio for Evaluating Audio Captioning Systems</h4>
    <p><strong>Satvik Dixit</strong>, Soham Deshmukh, Bhiksha Raj</p>
    <ul>
      <li>Under review at ICASSP 2025 Speech and Audio Language Models (SALMA) Workshop</li>
      <li><a href="https://arxiv.org/abs/2411.00321">Paper</a></li>
      <li><a href="https://github.com/satvik-dixit/mace/tree/main">Code</a></li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_3.png" alt="Vision Language Models Preview" class="publication-image" style="width: 300px;">
  <div class="publication-content">
    <h4>Vision Language Models Are Few-Shot Audio Spectrogram Classifiers</h4>
    <p><strong>Satvik Dixit</strong>, Laurie Heller, Chris Donahue</p>
    <ul>
      <li>Accepted at NeuRIPS Audio Imagination Workshop</li>
      <li><a href="https://openreview.net/pdf?id=RnBAclRKOC">Paper</a></li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_1.png" alt="Speaker Representations Preview" class="publication-image" style="width: 300px;">
  <div class="publication-content">
    <h4>Improving Speaker Representations Using Contrastive Losses on Multi-scale Features</h4>
    <p><strong>Satvik Dixit</strong>, Massa Baali, Rita Singh, Bhiksha Raj</p>
    <ul>
      <li>Under review at ICASSP 2025 (Main)</li>
      <li><a href="https://arxiv.org/abs/2410.05037">Paper</a></li>
      <li><a href="https://github.com/satvik-dixit/MFCon/tree/main">Code</a></li>
    </ul>
  </div>
</div>
<div class="publication">
  <img src="/assets/img/paper_2.png" alt="Speech Emotion Recognition Preview" class="publication-image" style="width: 300px;">
  <div class="publication-content">
    <h4>Explaining Deep Learning Embeddings for Speech Emotion Recognition by Predicting Interpretable Acoustic Features</h4>
    <p><strong>Satvik Dixit</strong>, Daniel M. Low, Gasser Elbanna, Fabio Catania, Satrajit S. Ghosh</p>
    <ul>
      <li>Under review at ICASSP 2025 (Main)</li>
      <li><a href="https://www.arxiv.org/abs/2409.09511">Paper</a></li>
      <li><a href="https://github.com/satvik-dixit/explainability_SER/tree/main">Code</a></li>
    </ul>
  </div>
</div>
<br>
<br>







 



  






